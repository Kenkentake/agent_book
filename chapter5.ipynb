{
 "cells": [
  {
   "cell_type": "code",
   "id": "v7akf6o5ul",
   "source": "# 環境変数の読み込み（必ず最初に実行すること）\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# 確認（APIキーの最初の数文字のみ表示）\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\ntavily_key = os.getenv(\"TAVILY_API_KEY\")\n\nif openai_key:\n    print(f\"✓ OPENAI_API_KEY が読み込まれました: {openai_key[:8]}...\")\nelse:\n    print(\"✗ OPENAI_API_KEY が設定されていません\")\n\nif tavily_key:\n    print(f\"✓ TAVILY_API_KEY が読み込まれました: {tavily_key[:8]}...\")\nelse:\n    print(\"✗ TAVILY_API_KEY が設定されていません\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3ea10",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n        (\"human\", \"{dish}\"),\n    ]\n)\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\noutput_parser = StrOutputParser()"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a27947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏肉（もも肉または胸肉）: 400g\n",
      "- 玉ねぎ: 2個\n",
      "- にんじん: 1本\n",
      "- じゃがいも: 2個\n",
      "- カレールー: 1箱（約200g）\n",
      "- サラダ油: 大さじ2\n",
      "- 水: 800ml\n",
      "- 塩: 適量\n",
      "- 胡椒: 適量\n",
      "- お好みでガーリックパウダーや生姜: 適量\n",
      "\n",
      "### 作り方\n",
      "1. **材料の下ごしらえ**:\n",
      "   - 鶏肉は一口大に切り、塩と胡椒をふっておきます。\n",
      "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
      "\n",
      "2. **炒める**:\n",
      "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
      "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
      "\n",
      "3. **野菜を加える**:\n",
      "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
      "\n",
      "4. **煮る**:\n",
      "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、中火にして蓋をし、約15分煮ます。\n",
      "\n",
      "5. **カレールーを加える**:\n",
      "   - 火を止めてカレールーを加え、よく溶かします。再び弱火にし、10分ほど煮込みます。お好みでガーリックパウダーや生姜を加えても良いです。\n",
      "\n",
      "6. **味を調える**:\n",
      "   - 最後に味を見て、必要であれば塩で調整します。\n",
      "\n",
      "7. **盛り付け**:\n",
      "   - ご飯を皿に盛り、その上にカレーをかけて完成です。お好みで福神漬けやらっきょうを添えても美味しいです。\n",
      "\n",
      "### おすすめのトッピング\n",
      "- チーズ\n",
      "- 生卵（温泉卵や目玉焼き）\n",
      "- 青ねぎやパセリの刻んだもの\n",
      "\n",
      "この基本のカレーはアレンジがしやすいので、野菜や肉を変えて自分好みのカレーを楽しんでください！\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97cc9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "output = chain.invoke({\"dish\": \"カレー\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb503cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏肉（もも肉または胸肉）: 400g\n",
      "- 玉ねぎ: 2個\n",
      "- にんじん: 1本\n",
      "- じゃがいも: 2個\n",
      "- カレールー: 1箱（約200g）\n",
      "- サラダ油: 大さじ2\n",
      "- 水: 800ml\n",
      "- 塩: 適量\n",
      "- 胡椒: 適量\n",
      "- お好みでガーリックパウダーや生姜: 適量\n",
      "\n",
      "### 作り方\n",
      "1. **材料の下ごしらえ**:\n",
      "   - 鶏肉は一口大に切り、塩と胡椒を振っておきます。\n",
      "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
      "\n",
      "2. **炒める**:\n",
      "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
      "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
      "\n",
      "3. **野菜を加える**:\n",
      "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
      "\n",
      "4. **煮る**:\n",
      "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、中火にして蓋をし、約15分煮ます。\n",
      "\n",
      "5. **カレールーを加える**:\n",
      "   - カレールーを割り入れ、よく溶かします。さらに10分ほど煮込み、全体がなじんだら味を見て、必要に応じて塩や胡椒で調整します。\n",
      "\n",
      "6. **仕上げ**:\n",
      "   - お好みでガーリックパウダーや生姜を加えて風味をアップさせます。火を止めて、少し冷ますと味がなじみます。\n",
      "\n",
      "7. **盛り付け**:\n",
      "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
      "\n",
      "### おすすめのトッピング\n",
      "- 煮卵\n",
      "- チーズ\n",
      "- パクチー\n",
      "\n",
      "このレシピを参考に、ぜひ美味しいカレーを作ってみてください！"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20de49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\\n\\n### 材料（4人分）\\n- 鶏肉（もも肉または胸肉）: 400g\\n- 玉ねぎ: 2個\\n- にんじん: 1本\\n- じゃがいも: 2個\\n- カレールー: 1箱（約200g）\\n- サラダ油: 大さじ2\\n- 水: 800ml\\n- 塩: 適量\\n- 胡椒: 適量\\n- お好みでガーリックパウダーや生姜: 適量\\n\\n### 作り方\\n1. **材料の下ごしらえ**:\\n   - 鶏肉は一口大に切り、塩と胡椒をふっておきます。\\n   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\\n\\n2. **炒める**:\\n   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\\n   - 鶏肉を加え、表面が白くなるまで炒めます。\\n\\n3. **野菜を加える**:\\n   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\\n\\n4. **煮る**:\\n   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、中火にして蓋をし、約15分煮ます。\\n\\n5. **カレールーを加える**:\\n   - カレールーを割り入れ、よく溶かします。さらに10分ほど煮込み、全体がなじんだら火を止めます。\\n\\n6. **味を調える**:\\n   - お好みで塩や胡椒で味を調整します。\\n\\n7. **盛り付け**:\\n   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\\n\\n### おすすめのトッピング\\n- 煮卵\\n- チーズ\\n- ほうれん草のソテー\\n\\nこの基本のカレーは、アレンジがしやすいので、好きな具材を加えて楽しんでください！', 'うどんのレシピをご紹介します。シンプルで美味しい「かけうどん」の作り方です。\\n\\n### 材料（2人分）\\n- うどん（乾燥または生）: 2玉\\n- だし汁: 600ml（昆布と鰹節で取ったものがベストですが、だしの素でも可）\\n- 醤油: 大さじ2\\n- みりん: 大さじ1\\n- 塩: 少々\\n- トッピング（お好みで）:\\n  - ネギ（小口切り）\\n  - 天かす\\n  - かまぼこ\\n  - ほうれん草や小松菜（茹でたもの）\\n  - たまご（生卵や温泉卵）\\n\\n### 作り方\\n1. **だし汁を作る**: 鍋にだし汁を入れ、醤油、みりん、塩を加えて中火で温めます。味を見て、好みに応じて調整してください。\\n\\n2. **うどんを茹でる**: 別の鍋にたっぷりの水を沸かし、うどんをパッケージの指示に従って茹でます。茹で上がったら、冷水でしっかりと洗い、ぬめりを取ります。\\n\\n3. **盛り付け**: 茹でたうどんを器に盛り、温めただし汁をかけます。\\n\\n4. **トッピング**: お好みのトッピングを加えます。ネギや天かす、かまぼこ、茹でた野菜、卵などをトッピングして完成です。\\n\\n### 提案\\n- 具材を変えることで、様々なバリエーションが楽しめます。例えば、鶏肉やきのこを加えた「鶏うどん」や、海鮮を使った「海鮮うどん」などもおすすめです。\\n- 冷たい「ざるうどん」も人気です。茹でたうどんを冷水でしめ、つけだれを用意して楽しんでください。\\n\\nぜひ、お試しください！']\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "outputs = chain.batch([{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802f4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee37a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで答えてください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44fca120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 + 2 * 3 の答えは **16** です。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b773ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a38aa27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello, world!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18a71ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "output = chain.invoke({\"input\": \"Hello, world!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf082ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'upper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text.upper()\n\u001b[32m      4\u001b[39m chain = prompt | model | upper\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m output = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:4857\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4842\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4843\u001b[39m \n\u001b[32m   4844\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4854\u001b[39m \n\u001b[32m   4855\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4859\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4861\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4863\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4864\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:4714\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4712\u001b[39m                 output = chunk\n\u001b[32m   4713\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4714\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4715\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4717\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mupper\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupper\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/agent_book/.venv/lib/python3.13/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'AIMessage' object has no attribute 'upper'"
     ]
    }
   ],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "chain = prompt | model | upper\n",
    "output = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511d809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c12cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
    "    for text in input_stream:\n",
    "        yield text.upper()\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | upper\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"Hello, world!\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "818757e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallel\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは非常に楽観的なアシスタントです。ユーザーの入力に対して楽観的な意見を述べてください。\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser\n",
    "\n",
    "perssimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは非常に悲観的なアシスタントです。ユーザーの入力に対して悲観的な意見を述べてください。\"), \n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = perssimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50f21f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimistic': '生成AIの進化は本当に素晴らしいですね！技術が進むにつれて、私たちの生活や仕事の仕方がどんどん便利になっています。クリエイティブなプロセスが加速され、新しいアイデアや作品が次々と生まれてくるのは、まさに未来の可能性を感じさせます。\\n'\n",
      "               '\\n'\n",
      "               '例えば、アートや音楽、文章作成など、さまざまな分野で生成AIが活躍しており、私たちの想像力を広げてくれています。これからも進化を続けることで、私たちの生活がより豊かで楽しいものになること間違いなしです！新しい技術がもたらすチャンスを楽しみにしましょう！',\n",
      " 'pessimistic': '生成AIの進化について考えると、確かに技術は進歩していますが、その一方で私たちの未来はますます不透明になっています。AIが人間の仕事を奪うリスクは高まる一方で、倫理的な問題や偏見のあるデータによる影響も無視できません。結局、私たちが作り出したものが私たち自身を脅かす存在になる可能性があるのです。進化するAIが本当に私たちの生活を豊かにするのか、それとも新たな問題を引き起こすだけなのか、疑問が残ります。'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic\": optimistic_chain,\n",
    "        \"pessimistic\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallelの出力をRunnableの入力に連結する\n",
    "\n",
    "synthesisze_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的なアシスタントです。2つの意見をまとめてください。\"),\n",
    "        (\"human\", \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic\": optimistic_chain,\n",
    "            \"pessimistic\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesisze_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da20107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}